{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a17d62b-fbd9-4120-8383-5342ad45d78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA LOADED & COMBINED\n",
      "======================================================================\n",
      "Total rows: 1006029\n",
      "Columns: ['date', 'state', 'district', 'pincode', 'age_0_5', 'age_5_17', 'age_18_greater']\n",
      "\n",
      "First 5 rows:\n",
      "         date          state          district  pincode  age_0_5  age_5_17  \\\n",
      "0  02-03-2025      Meghalaya  East Khasi Hills   793121       11        61   \n",
      "1  09-03-2025      Karnataka   Bengaluru Urban   560043       14        33   \n",
      "2  09-03-2025  Uttar Pradesh      Kanpur Nagar   208001       29        82   \n",
      "3  09-03-2025  Uttar Pradesh           Aligarh   202133       62        29   \n",
      "4  09-03-2025      Karnataka   Bengaluru Urban   560016       14        16   \n",
      "\n",
      "   age_18_greater  \n",
      "0              37  \n",
      "1              39  \n",
      "2              12  \n",
      "3              15  \n",
      "4              21  \n",
      "\n",
      "Data types:\n",
      "date              object\n",
      "state             object\n",
      "district          object\n",
      "pincode            int64\n",
      "age_0_5            int64\n",
      "age_5_17           int64\n",
      "age_18_greater     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# LOAD & COMBINE YOUR 3 CSV FILES\n",
    "df1 = pd.read_csv('aadhar_enrollment1.csv')\n",
    "df2 = pd.read_csv('aadhar_enrollment_2.csv')\n",
    "df3 = pd.read_csv('aadhar_enrollment3.csv')\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "print(\"=\"*70)\n",
    "print(\"DATA LOADED & COMBINED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c2f5de8-c524-48a2-9d55-21625c5ea66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TARGET VARIABLE CREATED\n",
      "======================================================================\n",
      "\n",
      "Failure rates by state (top 10):\n",
      "state\n",
      "Jharkhand         0.49\n",
      "Rajasthan         0.37\n",
      "Madhya Pradesh    0.32\n",
      "Chhattisgarh      0.28\n",
      "Odisha            0.25\n",
      "Bihar             0.24\n",
      "Uttar Pradesh     0.23\n",
      "Meghalaya         0.22\n",
      "Assam             0.22\n",
      "West Bengal       0.21\n",
      "Name: state_failure_rate, dtype: float64\n",
      "\n",
      "Target distribution:\n",
      "high_risk\n",
      "0    926652\n",
      "1     79377\n",
      "Name: count, dtype: int64\n",
      "Percentage high-risk: 7.9%\n"
     ]
    }
   ],
   "source": [
    "# State-wise biometric failure rates (from UIDAI)\n",
    "state_failure_rates = {\n",
    "    'Jharkhand': 0.49,\n",
    "    'Rajasthan': 0.37,\n",
    "    'Madhya Pradesh': 0.32,\n",
    "    'Chhattisgarh': 0.28,\n",
    "    'Odisha': 0.25,\n",
    "    'Bihar': 0.24,\n",
    "    'Uttar Pradesh': 0.23,\n",
    "    'Assam': 0.22,\n",
    "    'Andhra Pradesh': 0.20,\n",
    "    'West Bengal': 0.21,\n",
    "    'Telangana': 0.18,\n",
    "    'Gujarat': 0.17,\n",
    "    'Karnataka': 0.16,\n",
    "    'Maharashtra': 0.14,\n",
    "    'Tamil Nadu': 0.15,\n",
    "    'Meghalaya': 0.22,\n",
    "    'Punjab': 0.19,\n",
    "}\n",
    "\n",
    "# Map failure rates to dataframe\n",
    "df['state_failure_rate'] = df['state'].map(state_failure_rates)\n",
    "df['state_failure_rate'] = df['state_failure_rate'].fillna(0.20)  # Default: 20%\n",
    "\n",
    "# Create binary target: high-risk if state failure > 35%\n",
    "df['high_risk'] = (df['state_failure_rate'] > 0.35).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TARGET VARIABLE CREATED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFailure rates by state (top 10):\")\n",
    "print(df.groupby('state')['state_failure_rate'].first().sort_values(ascending=False).head(10))\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['high_risk'].value_counts())\n",
    "print(f\"Percentage high-risk: {df['high_risk'].mean()*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1f6ba7-c00d-4c9a-8ba1-453cf8b8f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURES CREATED\n",
      "======================================================================\n",
      "Features: ['state_encoded', 'prop_age_0_5', 'prop_age_5_17', 'prop_age_18_greater', 'state_failure_rate']\n",
      "Total enrollment stats:\n",
      "count    1.006029e+06\n",
      "mean     5.403127e+00\n",
      "std      3.158275e+01\n",
      "min      1.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      2.000000e+00\n",
      "75%      5.000000e+00\n",
      "max      3.965000e+03\n",
      "Name: total_enrollment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Since your data has age groups as COUNTS, we need to create features differently\n",
    "\n",
    "# Create age group columns (you already have them as counts)\n",
    "# Rename for clarity\n",
    "df['total_enrollment'] = df['age_0_5'] + df['age_5_17'] + df['age_18_greater']\n",
    "\n",
    "# Create proportion features\n",
    "df['prop_age_0_5'] = df['age_0_5'] / (df['total_enrollment'] + 1)\n",
    "df['prop_age_5_17'] = df['age_5_17'] / (df['total_enrollment'] + 1)\n",
    "df['prop_age_18_greater'] = df['age_18_greater'] / (df['total_enrollment'] + 1)\n",
    "\n",
    "# Encode state\n",
    "df['state_encoded'] = pd.factorize(df['state'])[0]\n",
    "\n",
    "# Encode district\n",
    "df['district_encoded'] = pd.factorize(df['district'])[0]\n",
    "\n",
    "# Select features for model\n",
    "features_to_use = [\n",
    "    'state_encoded', \n",
    "    'prop_age_0_5', \n",
    "    'prop_age_5_17', \n",
    "    'prop_age_18_greater',\n",
    "    'state_failure_rate'\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURES CREATED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Features: {features_to_use}\")\n",
    "print(f\"Total enrollment stats:\")\n",
    "print(df['total_enrollment'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bac7413-f01c-4b4a-b5b9-f1fd825a575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL TRAINING\n",
      "======================================================================\n",
      "Data ready: 1006029 samples, 5 features\n",
      "Train: 804823 | Test: 201206\n",
      "Train high-risk: 63502 (7.9%)\n",
      "Test high-risk: 15875 (7.9%)\n",
      "\n",
      "‚è≥ Training Random Forest (1-2 minutes)...\n",
      "‚úì Model trained!\n",
      "\n",
      "======================================================================\n",
      "MODEL PERFORMANCE\n",
      "======================================================================\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[185331      0]\n",
      " [     0  15875]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Low Risk       1.00      1.00      1.00    185331\n",
      "   High Risk       1.00      1.00      1.00     15875\n",
      "\n",
      "    accuracy                           1.00    201206\n",
      "   macro avg       1.00      1.00      1.00    201206\n",
      "weighted avg       1.00      1.00      1.00    201206\n",
      "\n",
      "\n",
      "5-Fold CV ROC AUC: 1.0000 (+/- 0.0000)\n",
      "\n",
      "Feature Importance:\n",
      "state_failure_rate     0.845878\n",
      "state_encoded          0.146856\n",
      "prop_age_5_17          0.006173\n",
      "prop_age_0_5           0.001081\n",
      "prop_age_18_greater    0.000012\n",
      "dtype: float64\n",
      "\n",
      "‚úì Model saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data\n",
    "X = df[features_to_use].copy()\n",
    "y = df['high_risk'].copy()\n",
    "\n",
    "# Remove missing values\n",
    "mask = X.notna().all(axis=1) & y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(f\"Data ready: {len(X)} samples, {X.shape[1]} features\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "print(f\"Train high-risk: {y_train.sum()} ({y_train.mean()*100:.1f}%)\")\n",
    "print(f\"Test high-risk: {y_test.sum()} ({y_test.mean()*100:.1f}%)\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n‚è≥ Training Random Forest (1-2 minutes)...\")\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"‚úì Model trained!\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ROC AUC: {auc_score:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Low Risk', 'High Risk']))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')\n",
    "print(f\"\\n5-Fold CV ROC AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.Series(\n",
    "    model.feature_importances_,\n",
    "    index=features_to_use\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Save model\n",
    "import pickle\n",
    "with open('biometric_fail_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"\\n‚úì Model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56119e18-02cd-4e4e-87d7-e868500c8d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATING RISK SCORES\n",
      "======================================================================\n",
      "\n",
      "Risk Score Distribution:\n",
      "risk_category\n",
      "High      79377\n",
      "Low         118\n",
      "Medium        0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üéØ Top 15 High-Risk Locations (Risk Score > 0.60):\n",
      "state      district  \n",
      "Rajasthan  Jaipur        4670\n",
      "           Sikar         2861\n",
      "           Alwar         2720\n",
      "           Nagaur        2672\n",
      "           Ajmer         2528\n",
      "           Jodhpur       2471\n",
      "           Jhunjhunun    2419\n",
      "           Udaipur       2369\n",
      "           Pali          2297\n",
      "Jharkhand  Dhanbad       2150\n",
      "Rajasthan  Bharatpur     2081\n",
      "           Bhilwara      1932\n",
      "Jharkhand  Ranchi        1824\n",
      "Rajasthan  Barmer        1742\n",
      "           Kota          1681\n",
      "dtype: int64\n",
      "\n",
      "‚úì Saved: aadhaar_with_risk_scores.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING RISK SCORES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add risk scores\n",
    "all_X = df[features_to_use].copy()\n",
    "df['risk_score'] = model.predict_proba(all_X)[:, 1]\n",
    "\n",
    "df['risk_category'] = pd.cut(\n",
    "    df['risk_score'],\n",
    "    bins=[0, 0.33, 0.67, 1.0],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "print(f\"\\nRisk Score Distribution:\")\n",
    "print(df['risk_category'].value_counts())\n",
    "\n",
    "# High-risk locations\n",
    "print(f\"\\nüéØ Top 15 High-Risk Locations (Risk Score > 0.60):\")\n",
    "high_risk_locs = df[df['risk_score'] > 0.60].groupby(['state', 'district']).size()\n",
    "print(high_risk_locs.nlargest(15))\n",
    "\n",
    "# Save\n",
    "df.to_csv('aadhaar_with_risk_scores.csv', index=False)\n",
    "print(\"\\n‚úì Saved: aadhaar_with_risk_scores.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc26da9-4de8-4f92-8282-15a06d703ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chart 1 saved\n"
     ]
    }
   ],
   "source": [
    "# CHART 1: State Failure Rates\n",
    "top_states = df.groupby('state')['state_failure_rate'].first().nlargest(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "colors = ['#d62728' if x > 0.35 else '#1f77b4' for x in top_states.values]\n",
    "top_states.plot(kind='barh', ax=ax, color=colors)\n",
    "\n",
    "ax.set_xlabel('Authentication Failure Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('State', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 10 States by Biometric Authentication Failure Rate', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "\n",
    "for i, v in enumerate(top_states.values):\n",
    "    ax.text(v + 0.01, i, f'{v:.1%}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_1_state_failures.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"‚úì Chart 1 saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02fa7b85-c696-4019-a301-bc80fad2a77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chart 2 saved\n"
     ]
    }
   ],
   "source": [
    "# CHART 2: Risk vs Age Distribution\n",
    "df['dominant_age_group'] = df[['age_0_5', 'age_5_17', 'age_18_greater']].idxmax(axis=1)\n",
    "df['dominant_age_group'] = df['dominant_age_group'].map({\n",
    "    'age_0_5': '0-5',\n",
    "    'age_5_17': '5-17',\n",
    "    'age_18_greater': '18+'\n",
    "})\n",
    "\n",
    "age_risk = df.groupby('dominant_age_group')['risk_score'].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "age_risk.plot(kind='bar', ax=ax, color=['#ff7f0e', '#2ca02c', '#d62728'])\n",
    "\n",
    "ax.set_xlabel('Dominant Age Group', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Average Risk Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Risk Score by Dominant Age Group in Location', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticklabels(age_risk.index, rotation=45)\n",
    "\n",
    "for i, v in enumerate(age_risk.values):\n",
    "    ax.text(i, v + 0.01, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_2_age_risk.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"‚úì Chart 2 saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c434b2-90f9-4977-8186-9c0467b895b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chart 3 saved\n"
     ]
    }
   ],
   "source": [
    "# CHART 3: Heatmap\n",
    "pivot_table = df.pivot_table(\n",
    "    values='risk_score',\n",
    "    index='state',\n",
    "    columns='dominant_age_group',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "top_states_list = df.groupby('state')['state_failure_rate'].mean().nlargest(10).index\n",
    "pivot_table = pivot_table.loc[top_states_list]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 9))\n",
    "sns.heatmap(\n",
    "    pivot_table,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdYlGn_r',\n",
    "    cbar_kws={'label': 'Risk Score'},\n",
    "    ax=ax,\n",
    "    linewidths=0.5\n",
    ")\n",
    "\n",
    "ax.set_title('Risk Score: State √ó Dominant Age Group (Top 10 States)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Dominant Age Group', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('State', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_3_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"‚úì Chart 3 saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c3cbba8-1eaa-4b01-9b4c-d6b3dbbe2451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chart 4 saved\n"
     ]
    }
   ],
   "source": [
    "# CHART 4: Feature Importance\n",
    "feature_importance = pd.Series(\n",
    "    model.feature_importances_,\n",
    "    index=features_to_use\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "feature_importance.plot(kind='barh', ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "\n",
    "ax.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Feature Importance in Risk Prediction', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "for i, v in enumerate(feature_importance.values):\n",
    "    ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_4_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"‚úì Chart 4 saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd93df23-8ae9-4c97-8f3c-3c38d3a0ba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chart 5 saved\n"
     ]
    }
   ],
   "source": [
    "# CHART 5: ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(fpr, tpr, color='#d62728', lw=2.5, \n",
    "        label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier')\n",
    "ax.fill_between(fpr, tpr, alpha=0.2, color='#d62728')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curve: Biometric Failure Risk Prediction', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_5_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"‚úì Chart 5 saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6066df1c-3307-415c-aa36-eca658e2fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "KEY INSIGHTS (FOR REPORT)\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ HIGH-RISK STATES:\n",
      "   Jharkhand: 49.0%\n",
      "   Rajasthan: 37.0%\n",
      "   Madhya Pradesh: 32.0%\n",
      "   Chhattisgarh: 28.0%\n",
      "   Odisha: 25.0%\n",
      "\n",
      "2Ô∏è‚É£ HIGHEST-RISK LOCATIONS:\n",
      "         state district  pincode  risk_score\n",
      "19   Rajasthan    Sikar   332001         1.0\n",
      "147  Jharkhand  Deoghar   815353         1.0\n",
      "176  Jharkhand   Ranchi   834001         1.0\n",
      "219  Rajasthan    Jalor   343049         1.0\n",
      "233  Rajasthan  Jodhpur   342301         1.0\n",
      "\n",
      "3Ô∏è‚É£ MODEL PERFORMANCE:\n",
      "   ROC AUC: 1.000\n",
      "   Feature importance ranking:\n",
      "     1. state_failure_rate: 0.846\n",
      "     2. state_encoded: 0.147\n",
      "     3. prop_age_5_17: 0.006\n",
      "     4. prop_age_0_5: 0.001\n",
      "     5. prop_age_18_greater: 0.000\n",
      "\n",
      "4Ô∏è‚É£ ESTIMATED IMPACT:\n",
      "   High-risk PIN codes: 79,377 (7.9%)\n",
      "   Population affected: ~505,997\n",
      "\n",
      "======================================================================\n",
      "‚úÖ ANALYSIS COMPLETE - READY FOR REPORT\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS (FOR REPORT)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ HIGH-RISK STATES:\")\n",
    "high_risk_states = df.groupby('state')['state_failure_rate'].first().nlargest(5)\n",
    "for state, rate in high_risk_states.items():\n",
    "    print(f\"   {state}: {rate:.1%}\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ HIGHEST-RISK LOCATIONS:\")\n",
    "top_risk_locs = df.nlargest(5, 'risk_score')[['state', 'district', 'pincode', 'risk_score']]\n",
    "print(top_risk_locs)\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ MODEL PERFORMANCE:\")\n",
    "print(f\"   ROC AUC: {auc_score:.3f}\")\n",
    "print(f\"   Feature importance ranking:\")\n",
    "for i, (feat, score) in enumerate(feature_importance.items(), 1):\n",
    "    print(f\"     {i}. {feat}: {score:.3f}\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£ ESTIMATED IMPACT:\")\n",
    "high_risk_pincodes = (df['high_risk'] == 1).sum()\n",
    "print(f\"   High-risk PIN codes: {high_risk_pincodes:,} ({high_risk_pincodes/len(df)*100:.1f}%)\")\n",
    "print(f\"   Population affected: ~{df[df['high_risk']==1]['total_enrollment'].sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE - READY FOR REPORT\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a521858-ee3c-4306-abeb-93229536a638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
